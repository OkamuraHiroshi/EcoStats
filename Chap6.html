<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
</head>
 
<body>

## 6 - 1 個体数推定のための調査手法

possum <- read.csv("https://raw.githubusercontent.com/cran/PL.popN/master/data/possum.txt", sep=" ")       # インターネットネット上にあるpossum.txtを取り込む
summary(possum)       # possumの中身を確認

possum$n <- 5-possum$t1     # possumデータから二項分布の試行回数を作成
possum$z <- possum$y-1      # possumデータから二項分布の成功回数を作成
mod0 <- glm(cbind(z,n-z)~1,family=binomial,data=possum)      # 二項分布回帰を実行

logit_p <- mod0$coefficients       # 二項分布回帰で推定された回帰係数
ilogit <- function(x) exp(x)/(1+exp(x))         # logit値を逆変換して確率に戻す関数
( p <- ilogit(logit_p) )       # possumの発見確率

n <- nrow(possum)        # possumのサンプルサイズ
( N <- n/p )        # サンプルサイズを発見確率で割ることで個体数を推定する

library(tidyverse)        # tidyverseパッケージの読み込み
mod1 <- glm(cbind(z,n-z)~x,family=binomial,data=possum)         # 体重量xを説明変数として二項分布回帰する
mod2 <- glm(cbind(z,n-z)~x+I(x^2),family=binomial,data=possum)                 # 体重量xと体重量x^2を説明変数として二項分布回帰する
AIC(mod0, mod1, mod2)          # 説明変数なし，説明変数x，説明変数x + x^2のモデルのAIC比較
new_x <- 30:50         # グラフで予測曲線を描くためのx軸の値
new_possum <- data.frame(x=new_x,y=ilogit(predict(mod2,newdata=list(x=new_x))))
ggplot(new_possum, aes(x,y))+geom_line()+labs(x="Body Weight (g)", y="Detection Probability")+theme_bw()        # AIC最小のモデルmod2による予測結果のプロット

pred_p <- ilogit(predict(mod2))     # mod2を使った場合の発見確率（体重により異なる）
( N_new <- sum(1/pred_p) )        # 体重による発見確率の違いを考慮した個体数推定

numeric_deriv <- function(mod, h=0.00001){     # 個体数推定値のパラメータによる微分を計算する関数
  p <- mod$coef      # 回帰のパラメータ
  X <- model.matrix(mod)        # 説明変数
  d <- h*diag(length(p))        # 数値微分のために回帰係数に足し引きする微小量を与える
  (apply(d, 1, function(x) sum(1/ilogit(X%*%(p+x))))-apply(d, 1, function(x) sum(1/ilogit(X%*%(p-x)))))/(2*h)      # 各パラメータに対する個体数推定値の微分
}
var_N <- t(numeric_deriv(mod2))%*%vcov(mod2)%*%numeric_deriv(mod2)       # デルタ法による個体数の近似分散
se_N <- sqrt(var_N)        # 個体数の標準誤差
( cv_N <- se_N/N_new )       # 個体数の変動係数

alpha <- 1-0.95        # 有意水準の設定
C_ln <- exp(qnorm(1-alpha/2)*sqrt(log(1+cv_N^2)))       # 対数正規分布の近似信頼区間で使用する値
CI_ln <- c(N_new/C_ln, N_new*C_ln)        # 対数正規分布の信頼区間
round(CI_ln, 2)       # 信頼区間を表示

alpha <- 1-0.95       # 有意水準の設定
Sim <- 2000       # シミュレーションの回数
N_b <- NULL       # bootstrap sampleに対する個体数推定値
set.seed(1)       # 乱数発生のseed設定
for (i in 1:Sim){
  id <- sample(n, n, replace=TRUE)        # n個からn個を復元抽出でリサンプル
  mod2b <- glm(cbind(z,n-z)~x+I(x^2),family=binomial,dat=possum[id,])       # bootsrapサンプルデータに対して2次の共変量を持つ二項回帰モデルをフィット
  pred_pb <- ilogit(predict(mod2b))        # 発見確率の推定
  N_b <- c(N_b, sum(1/pred_pb))         # bootstrapによる個体数推定値を保存
}
CI_b <- quantile(N_b,probs=c(alpha/2,1-alpha/2))       # percentile法による信頼区間
round(CI_b, 2)        # 信頼区間の表示

library(Rdistance)          # Rdistanceパッケージの読み込み
data("sparrowDetectionData")         # sparrowDetectionDataの読み込み
dat <- sparrowDetectionData          # sparrowDetectionDataをdatにコピー
dat <- dat %>% mutate(pd=as.numeric(dist))         # pd（垂直横距離を数値化したもの）を新しい変数として追加
p1 <- ggplot(dat, aes(x=pd, y=..density..))+geom_histogram(position="identity",boundary=0,bins=15)+labs(x="Perpendicular Distance (m)")+theme_bw()      # 垂直横距離の確率分布の図
print(p1)       # 図を表示

x_max <- 120       # 横距離の最大値
dat <- dat %>% filter(pd <= 120)       # 横距離を120m以下のものに制限する
p1 <- ggplot(dat, aes(x=pd, y=..density..))+geom_histogram(position="identity",boundary=0, bins=15)+labs(x="Perpendicular Distance (m)")+theme_bw()      # 横距離の確率分布の図
g <- function(x,sigma) exp(-x^2/(2*sigma^2))       # 半正規分布の発見関数
log_like <- function(p, dat, x_max=Inf){        # 発見確率のパラメータ推定のための対数尤度関数
  x <- dat$pd       # 垂直横距離
  x <- x[x <= x_max]        # x_max以下の距離データに制限（truncationと言う）
  sigma <- exp(p)       # 発見確率のパラメータ sigma
#  w <- integrate(g,0,w,sigma=sigma)$value        # 有効探索幅の計算（一般には解析的に求められないので，数値積分などで計算する．発見関数が半正規分布の場合は，pnormで計算可能）
  w <- sqrt(2*pi)*sigma*(pnorm(x_max,0,sigma)-pnorm(0,0,sigma))       # 発見関数が半正規分布のときの有効探索幅
  -sum(log(g(x,sigma))-log(w))       # 横距離データに対する負の対数尤度
}
init_p <- log(mean(dat$pd))      #   発見確率のパラメータ（sigmaの対数値）の初期値（平均横距離の対数を与える）
mod <- nlm(log_like, init_p, dat=dat, x_max=x_max, hessian=TRUE)        # パラメータ推定の最適化
sigma <- exp(mod$estimate)       # 推定結果から発見関数のパラメータsigmaを推定
x <- seq(0,x_max)         # 横距離の範囲
esw <- function(sigma, x_max=Inf) sqrt(2*pi)*sigma*(pnorm(x_max,0,sigma)-pnorm(0,0,sigma))        # 有効探索幅を計算する関数
w <- esw(sigma, x_max)           # 推定パラメータと横距離の最大値から有効探索幅を計算
dat1 <- layer_data(p1)          # 上で作ったggplotのグラフp1から元となるデータを抽出
pred <- sapply(1:nrow(dat1), function(i) sqrt(2*pi)*sigma*(pnorm(dat1$xmax[i],0,sigma)-pnorm(dat1$xmin[i],0,sigma)))         # 垂直横距離の理論的な確率予測値を計算する
pred1 <- pred/sum(pred)*sum(dat1$y)        # グラフで観測度数と予測値の高さを合わせるための調整
p2 <- p1 + geom_line(data=dat1, aes(x=x,y=pred1), color="blue")      # 横距離の元の観測値頻度グラフにモデルから予測された曲線を追加フィットするグラフ
print(p2)        # グラフ描画

data(sparrowSiteData)        # 発見場所に関するライントランセクト調査データ
dat_s <- sparrowSiteData         # sparrowSiteDataをdat_sにコピー
L <- sum(as.numeric(dat_s$length))         # 調査の総距離
n <- nrow(dat)       # 総発見数
A <- 4105*1000^2         # 調査面積（?sparrowSiteDataから情報を得る）
( N <- A*n/(2*L*w) )        # 個体数推定値

numeric_deriv <- function(mod, h=0.00001){       # 有効探索幅のパラメータによる数値微分を行う関数
  p <- mod$estimate      # 最適化で推定されたパラメータ
  d <- h*diag(length(p))         # 数値微分のための微小量の設定
  (apply(d, 1, function(x) esw(exp(p+x)))-apply(d, 1, function(x) esw(exp(p-x))))/(2*h)       # 数値微分
}
var_w <- t(numeric_deriv(mod))%*%(1/mod$hessian)%*%numeric_deriv(mod)       # デルタ法による有効探索幅の分散の計算
cv_w <- sqrt(var_w)/w        # 有効探索幅の変動係数
k <- nrow(dat_s)        # 調査ラインの数
n <- tapply(dat$dist, dat$siteID, length)      # ラインごとの発見数
n[is.na(n)] <- 0       # そのラインに発見がない場合，NAを発見数=0にする
l <- as.numeric(dat_s$length)        # 各ラインの長さ
mean_nl <- sum(n)/L         # 全ラインあたりの発見数（遭遇率）
var_nl <- 1/(k-1)*sum(l/L*(n/l-mean_nl)^2)         # 遭遇率のライン重み付き分散
cv_nl <- sqrt(var_nl)/mean_nl       # 遭遇率の変動係数
cv_N <- sqrt(cv_nl^2+cv_w^2)       # 遭遇率の変動係数の二乗と有効探索幅の変動係数の二乗を合わせて平方根をとることにより個体数の変動係数を計算
C_ln <- exp(qnorm(1-0.05/2)*sqrt(log(1+cv_N^2)))        # 対数正規信頼区間に必要な定数
CI_ln <- c(N/C_ln, N*C_ln)       # 対数正規分布を仮定した信頼区間
round(CI_ln, 2)       # 信頼区間の表示

## 6 - 2　占有モデル

library(unmarked)        # unmarkedパッケージの読み込み
data(crossbill)         # crossbillデータの読み込み
dat <- crossbill[, 1:7]          # crossbillデータをdatにコピー
dat$n <- apply(dat[,5:7],1,function(x) sum(!is.na(x)))        # 調査の回数（基本は3回調査だが，全く調査がされていなかったり，最後の調査が欠測していて2回だけとなっている場合もある）
dat$x <- apply(dat[,5:7],1,sum,na.rm=TRUE)       # 発見があった調査の個数
dat <- dat[dat$n > 0, ]       # 調査回数が正のデータだけとってくる

loglik <- function(p, dat){         # （負の）対数尤度関数
  psi <- exp(p[1])        # 存在確率
  theta <- exp(p[2])        # 発見確率
  n <- dat$n       # 調査回数
  x <- dat$x       # 調査のうち発見があった回数
  -sum(log(dbinom(x,n,theta)*psi+ifelse(x==0,1,0)*(1-psi)),na.rm=TRUE)      # 負の対数尤度（存在していたら発見があったときに観測されるが，存在していなかったら発見もなし：0は膨らませられている）
}
mod <- nlm(loglik,c(-1,-1),dat)       # 最適化（負の対数尤度を最小にするパラメータを探索）
psi <- exp(mod$estimate[1])        # 存在確率
theta <- exp(mod$estimate[2])        # 発見確率
( parms <- c(psi=psi, theta=theta) )         # 推定パラメータをまとめて表示

i0 <- which(dat$x==0)        # 発見が0になっているデータを抽出
p0 <- dbinom(0,dat$n[i0],theta)*psi/(dbinom(0,dat$n[i0],theta)*psi+(1-psi))         # 見落とされていた（x=0）のうち実際には存在していたという確率（ベイズの定理による事後確率）を計算
N0 <- replicate(1000,sum(rbinom(length(i0),1,p0)))       # 見落とされたが存在したものを確率的に復元
N <- sum(dat$x>0)+N0        # 発見されたものに見落とされたものを足してやる
( quantile(N, probs=c(0.025,0.5,0.975)) )          # 復元全個体数の信頼区間
hist(N)       # 復元全個体数のヒストグラム

n_mix <- function(p, dat, max_N=100){        # N-mixtureモデルの（負の）対数尤度関数を計算する関数
  x <- dat$x       # 発見数
  n <- dat$n       # 調査数
  phi <- 1/(1+exp(-p[1]))        # 1個体を発見する確率
  theta <- function(N) 1-(1-phi)^N         # N個体が存在するとき，少なくとも1個体が発見される確率
  lambda <- exp(p[2])        # 平均個体数
  
  like <- sapply(1:nrow(dat), function(i) sum(dbinom(x[i],n[i],theta(0:max_N))*dpois(0:max_N,lambda)))          # 平均個体数がlambdaのポアソン分布に従うとして，N個体いるとき，発見確率がtheta(N)であるときに，n調査中でx回発見されるとなる確率であるが，Nは観測されないので和をとることになる．
  -sum(log(like))           # 負の対数尤度
}
mod <- nlm(n_mix, c(-1,-1), dat=dat, hessian=TRUE)       # 最適化によるN混合モデルの個体数推定
phi <- 1/(1+exp(-mod$estimate[1]))       # 個体の発見確率
lambda <- exp(-mod$estimate[2])       # 平均個体数
( c(phi=phi, lambda=lambda) )       # パラメータの表示

N <- nrow(dat)*lambda        # 総個体数の推定
var_N <- nrow(dat)^2*as.numeric(t(c(0,lambda))%*%solve(mod$hessian)%*%c(0,lambda))       # 個体数の分散推定量（デルタ法）
cv_N <- sqrt(var_N)/N        # 個体数の変動係数
C_ln <- exp(qnorm(1-0.05/2)*sqrt(log(1+cv_N^2)))        # 対数正規の信頼区間に必要な量
CI_ln <- c(lower=N/C_ln, estimate=N, upper=N*C_ln)          # 対数正規信頼区間（最尤推定値も含む）
round(CI_ln, 2)       # 信頼区間表示

## 6 - 3　相対資源量指数

# CPUE標準化の重要性を示すシミュレーション
set.seed(1)       # 乱数発生のseed設定
a <- exp(-0.2); log_a <- log(a)       # a（年による減少率）とその対数
Y <- 30; Sim <- 100; SS <- 5; sigma <- 0.4; q <- c(0.1, 0.9)       # パラメータ設定（Y：年数，Sim：シミュレーション回数，SS：各年の観測数，sigma：CPUEの対数値の標準偏差，q：東西の漁獲効率
Year <- rep(rev(2022-(0:(Y-1))), each=SS)      # 1993年から2022年まで30年間の年ラベル（各年5個ずつ）
n <- matrix(NA, nrow=Y, ncol=Sim)        # 個体数の対数値の入れ物となる行列
cpue <- log_q <- matrix(NA, nrow=Y*SS, ncol=Sim)        # CPUEと漁具能率の対数値の行列
n[1,] <- rnorm(Sim,0,1)         # 1年前の個体数
eps <- matrix(sigma*rnorm(Y*SS*Sim),nrow=Y*SS,ncol=Sim)       # cpueの観測誤差
p <- function(i) 1/(1+exp(-(i-Y/2)/5))       # 西側で漁獲を行う確率（年によって変化する）
Location <- sapply(rep(1:Y,each=SS), function(i) rbinom(Sim, 1, p(i)))       # 上で定義した確率で選択された漁獲を行う場所（Locationは，行数がSim，列数がY*SSになっている）
log_q <- log(t(matrix(q[Location+1],nrow=Sim,ncol=Y*SS))); Location <- t(Location)      # 東西場所選択に対応したqの選択とlog_qの作成（転置して，Y*SS行Sim列に）．後のために，Locationも転置してY*SS行Sim列の行列に．
cpue[1:SS,] <- log_q[1:SS,] + n[1,] + eps[1:SS,]        # 最初の年のCPUE
for (i in 1:(Y-1)){
  n[i+1,] <- log_a+n[i,]       # i年のCPUE（年率aで減少していっている）
  cpue[i*SS+(1:SS),] <- log_q[i*SS+1:SS,] + n[i+1,] + eps[i*SS+1:SS,]        # 個体数に漁獲効率qを掛け，誤差を付与してcpue（対数値）を作成
}
p1 <- ggplot(data=data.frame(Year=unique(Year), Prob=p(1:Y)),aes(x=Year,y=Prob))+geom_line(color="blue",linewidth=2)+labs(y="Probability of Selecting q = 0.9")+theme_bw()        # 年による場所選択率の変化の様子（年が進むと西に行きやすくなる）
WE <- c("East","West")       # 東西のラベル
Loc <- WE[Location[,1]+1]         # 選択された場所が東西どちらだったか
dat_for_gg <- data.frame(Year=rep(unique(Year),each=SS), num=rep(1:SS,len=nrow(cpue)), Location=Loc, CPUE=cpue[,1])        # プロットに必要なデータをまとめる
p2 <- ggplot(dat_for_gg, aes(x=Year, y=CPUE, color=Location))+labs(y="log(CPUE)")+geom_point()+theme_bw()+theme(legend.position = c(0.1, 0.25))         # 年に対するCPUEの対数値をプロット（東西どちらで漁獲を行っていたかを色の違いで描画）
cowplot::plot_grid(p1,p2,nrow=2,align="v")         # 上で作った選択率変化のグラフとCPUE変化の様子を上下に並べて表示

res_nom <- lapply(1:Sim, function(i) lm(cpue[,i]~Year))       # 場所の変化による漁獲効率の違いを無視して減少率を推定するモデル
res_std <- lapply(1:Sim, function(i) lm(cpue[,i]~Year*Location[,i]))         # 年と場所の交互作用を考えることにより，年による場所の違いを考慮に入れたモデル
dat_for_gg2 <- data.frame(Model=c(rep(c("Nominal","Standardized"),each=Sim)), Trend=c(sapply(1:Sim, function(i) res_nom[[i]]$coef[2]), sapply(1:Sim, function(i) res_std[[i]]$coef[2])))       # 2つのモデルから推定された減少率の推定値を並べる
ggplot(dat_for_gg2,aes(x=Model,y=Trend))+geom_boxplot()+geom_hline(yintercept=log_a,linetype="dashed")+theme_bw()      # 2つのモデルによって推定された減少率の対数値と真の減少率の対数値を比較

library(glmmTMB)       # glmmTMBパッケージの読み込み
library(rvest)       # rvestパッケージの読み込み
library(stringr)       # stringrパッケージの読み込み
html <- read_html("https://iotc.org/data/datasets/latest/CE/Longline")       # はえ縄で漁獲されたまぐろの漁獲情報が載っているウェブサイトの場所
zip_loc <- html %>% html_elements("a") %>% html_attr("href") %>% str_subset("\\.zip")       # 上記ウェブサイトにあるzipファイルを抽出
temp <- tempfile()      # 空のファイル名を設定
download.file(zip_loc, temp)        # zipファイルをtempとしてダウンロード
cpue_dat <- read.csv(unzip(temp))         # tempを解凍して，中のcsvファイルを読み込む
YFT_dat <- cpue_dat[,c(1:12,13:15)]          # 必要なデータの抽出
YFT_dat$Catch <- apply(YFT_dat[,13:15],1,sum,na.rm=TRUE)         # 漁獲量の列を作成
dat <- subset(YFT_dat, Year >= 2000 & substr(YFT_dat$Fleet,1,3)=="JPN")        # 2000年以降日本が漁獲したデータに限定してdatとする
dat$Year <- factor(dat$Year)
dat$Month <- (dat$MonthStart+dat$MonthEnd)/2
dat$Month <- factor(dat$Month)
dat$Grid <- factor(dat$Grid)
mod_td0 <- glmmTMB(Catch~ Year-1,offset=log(Effort),family=tweedie,data=dat)
mod_td1 <- glmmTMB(Catch~ Year+(1|Month)+(1|Grid)-1,offset=log(Effort),family=tweedie,data=dat)
mod_td2 <- glmmTMB(Catch~ Year+(1|Month)+(1|Grid)+(1|Year:Month)+(1|Year:Grid)-1,offset=log(Effort),family=tweedie,data=dat)
AIC(mod_td0,mod_td1,mod_td2)



</body>
</html>
